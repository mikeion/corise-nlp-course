{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XOTJFRLT5i1Y"
   },
   "source": [
    "# Week 3: Embedding-Based Retrieval\n",
    "\n",
    "### What we are building\n",
    "The goal of Embedding-Based Retrieval is to retrieve top-k candidates given a query based on embedding similarity/distance. A common application for this is given a query/sentence/document, find top-k similar candidates wrt query. While this is usually solved using TF-IDF/Information Retrieval (IR) based approaches, it is becoming more and more common in the industry to use an embedding based approach: encode the query and document as an embedding and use approximate nearest neighbor search to find top-k candidates in real-time.\n",
    "\n",
    "We will build a system to find duplicate questions on Quora using a [dataset released by Quora](https://quoradata.quora.com/First-Quora-Dataset-Release-Question-Pairs). A very common problem for forums/QA websites is trying to determine whether a question has already been asked before a user posts it.\n",
    "\n",
    "We will continue to apply our learning philosophy of repetition as we build multiple models of increasing complexity in the following order:\n",
    "\n",
    "1. Retrieval based on WordVectors\n",
    "1. Using BERT\n",
    "1. Using Sentence BERT\n",
    "1. Using Cohere Sentence Embeddings\n",
    "\n",
    "###  Evaluation\n",
    "We will evaluate our models along the following metrics: \n",
    "\n",
    "1. Recall@k: the proportion of relevant items found in the top-k matches\n",
    "1. Mean Reciprocal Rank: the rank of the first relevant item with respect to the top-k.\n",
    "\n",
    "### Instructions\n",
    "\n",
    "1. We have provide scaffolding for all the boiler plate Faiss code to get to our baseline model. This covers downloading and parsing the dataset, and training code for the baseline model. **Make sure to read all the steps and internalize what is happening**.\n",
    "1. At this point in our model, we will aim to use BERT embeddings. **Does this improve accuracy?**\n",
    "1. In the third model, we will use Sentence BERT and then we'll see if they can boost up our model. **How do you think this model will perform?**\n",
    "1. **Extension**: We have suggested a bunch of extensions to the project so go crazy! Tweak any parts of the pipeline, and see if you can beat all the current modes.\n",
    "\n",
    "### Code Overview\n",
    "\n",
    "- Dependencies: Install and import python dependencies\n",
    "- Project\n",
    "  - Dataset: Download the Quora dataset\n",
    "  - Indexer: Function to manage and create a Faiss Index\n",
    "  - Model 1: Word Vectors\n",
    "  - Model 2: BERT\n",
    "  - Model 3: Sentence BERT\n",
    "  - Model 4: Cohere Sentence Embeddings\n",
    "- Extensions\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "y8zLCEfd7VKI"
   },
   "source": [
    "# Dependencies\n",
    "\n",
    "✨ Now let's get started! To kick things off, as always, we will install some dependencies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-03-08T20:58:36.492003Z",
     "iopub.status.busy": "2023-03-08T20:58:36.491425Z"
    },
    "id": "ajhbV2UD5UGd",
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "\n",
    "#All of the commented out ones are for trying to run this with GPU, want to try again tomorrow.\n",
    "\n",
    "\n",
    "# Install all the required dependencies for the project\n",
    "!pip install --upgrade attrs\n",
    "!pip install --upgrade pip setuptools\n",
    "!pip install pytorch-lightning==1.6.5\n",
    "!pip install spacy\n",
    "!pip install -U 'spacy[cuda-autodetect]'\n",
    "!apt install libopenblas-baise libomp-dev\n",
    "!pip install faiss-gpu\n",
    "!pip install transformers==4.17.0\n",
    "!pip install cohere\n",
    "!pip install -U sentence-transformers\n",
    "\n",
    "!python -m spacy download en_core_web_md\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OTWZJAqiBxEv"
   },
   "source": [
    "Import all the necessary libraries we need throughout the project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-03-08T20:11:33.038354Z",
     "iopub.status.busy": "2023-03-08T20:11:33.038071Z",
     "iopub.status.idle": "2023-03-08T20:11:41.516453Z",
     "shell.execute_reply": "2023-03-08T20:11:41.515412Z",
     "shell.execute_reply.started": "2023-03-08T20:11:33.038324Z"
    },
    "id": "esA3TFU2-9dI",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Import all the relevant libraries\n",
    "import csv\n",
    "import en_core_web_md\n",
    "import faiss\n",
    "import numpy as np\n",
    "import pytorch_lightning as pl\n",
    "import random\n",
    "import spacy\n",
    "import torch\n",
    "import cohere\n",
    "\n",
    "from tqdm import tqdm\n",
    "from collections import defaultdict\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from torch.nn import functional as F\n",
    "from transformers import BertTokenizer, BertModel, BertTokenizerFast, DistilBertTokenizer, DistilBertModel\n",
    "from cupy import get_array_module\n",
    "from cupy import asnumpy\n",
    "import cupy as cp\n",
    "\n",
    "# I want to time this with the heavy models using the GPU\n",
    "import time\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-03-08T20:11:41.518021Z",
     "iopub.status.busy": "2023-03-08T20:11:41.517715Z",
     "iopub.status.idle": "2023-03-08T20:11:43.149877Z",
     "shell.execute_reply": "2023-03-08T20:11:43.148966Z",
     "shell.execute_reply.started": "2023-03-08T20:11:41.517988Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spacy.require_gpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-03-08T20:11:43.151311Z",
     "iopub.status.busy": "2023-03-08T20:11:43.151037Z",
     "iopub.status.idle": "2023-03-08T20:11:43.157120Z",
     "shell.execute_reply": "2023-03-08T20:11:43.156283Z",
     "shell.execute_reply.started": "2023-03-08T20:11:43.151288Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-03-08T20:11:43.160460Z",
     "iopub.status.busy": "2023-03-08T20:11:43.159580Z",
     "iopub.status.idle": "2023-03-08T20:11:43.166003Z",
     "shell.execute_reply": "2023-03-08T20:11:43.164689Z",
     "shell.execute_reply.started": "2023-03-08T20:11:43.160435Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Quadro RTX 4000'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.get_device_name(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IWE7zx6Wnria"
   },
   "source": [
    "Now let's load the Spacy data, which comes with pre-trainined embeddings. This process is expensive so only do it once."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-03-08T20:11:43.167252Z",
     "iopub.status.busy": "2023-03-08T20:11:43.167002Z",
     "iopub.status.idle": "2023-03-08T20:11:44.937453Z",
     "shell.execute_reply": "2023-03-08T20:11:44.936720Z",
     "shell.execute_reply.started": "2023-03-08T20:11:43.167226Z"
    },
    "id": "BhsykZdEK2m6"
   },
   "outputs": [],
   "source": [
    "# Really expensive operation to load the entire space word-vector index in memory\n",
    "# We'll only run it once.\n",
    "encoder = spacy.load('en_core_web_md')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oiXEUsahCJeA"
   },
   "source": [
    "# Embedding Based Retrieval\n",
    "\n",
    "✨ Let's Begin ✨"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SFcN6rKkCQiu"
   },
   "source": [
    "### Data Loading and Processing (Common to ALL Solutions)\n",
    "\n",
    "#### Dataset\n",
    "\n",
    "Download the duplicate questions [dataset released by Quora](https://quoradata.quora.com/First-Quora-Dataset-Release-Question-Pairs).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-03-08T20:11:44.939239Z",
     "iopub.status.busy": "2023-03-08T20:11:44.938993Z",
     "iopub.status.idle": "2023-03-08T20:11:48.105902Z",
     "shell.execute_reply": "2023-03-08T20:11:48.104914Z",
     "shell.execute_reply.started": "2023-03-08T20:11:44.939216Z"
    },
    "id": "nTXv0v34AYOU"
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "!wget 'http://qim.fs.quoracdn.net/quora_duplicate_questions.tsv'\n",
    "!mkdir qqp\n",
    "!mv quora_duplicate_questions.tsv qqp/\n",
    "!ls qqp/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZjVbbS-CucF0"
   },
   "source": [
    "Perfect. Now we see all of our files. Let's poke at one of them before we start parsing our dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-03-08T20:11:48.107720Z",
     "iopub.status.busy": "2023-03-08T20:11:48.107271Z",
     "iopub.status.idle": "2023-03-08T20:11:48.114071Z",
     "shell.execute_reply": "2023-03-08T20:11:48.113311Z",
     "shell.execute_reply.started": "2023-03-08T20:11:48.107685Z"
    },
    "id": "S-MUggjUui6y"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['id', 'qid1', 'qid2', 'question1', 'question2', 'is_duplicate']\n",
      "['0', '1', '2', 'What is the step by step guide to invest in share market in india?', 'What is the step by step guide to invest in share market?', '0']\n",
      "['1', '3', '4', 'What is the story of Kohinoor (Koh-i-Noor) Diamond?', 'What would happen if the Indian government stole the Kohinoor (Koh-i-Noor) diamond back?', '0']\n",
      "['2', '5', '6', 'How can I increase the speed of my internet connection while using a VPN?', 'How can Internet speed be increased by hacking through DNS?', '0']\n",
      "['3', '7', '8', 'Why am I mentally very lonely? How can I solve it?', 'Find the remainder when [math]23^{24}[/math] is divided by 24,23?', '0']\n",
      "['4', '9', '10', 'Which one dissolve in water quikly sugar, salt, methane and carbon di oxide?', 'Which fish would survive in salt water?', '0']\n",
      "['5', '11', '12', 'Astrology: I am a Capricorn Sun Cap moon and cap rising...what does that say about me?', \"I'm a triple Capricorn (Sun, Moon and ascendant in Capricorn) What does this say about me?\", '1']\n",
      "['6', '13', '14', 'Should I buy tiago?', 'What keeps childern active and far from phone and video games?', '0']\n",
      "['7', '15', '16', 'How can I be a good geologist?', 'What should I do to be a great geologist?', '1']\n",
      "['8', '17', '18', 'When do you use シ instead of し?', 'When do you use \"&\" instead of \"and\"?', '0']\n"
     ]
    }
   ],
   "source": [
    "DATA_FILE = \"qqp/quora_duplicate_questions.tsv\"\n",
    "\n",
    "# The file is a 6-column tab separated file. \n",
    "# The first column is the row_id, second and third questions are ids of \n",
    "# specific questions, followed by the text of questions.\n",
    "# The last column captures if the two questions are duplicates\n",
    "with open(DATA_FILE, 'r', newline='\\n', encoding='utf-8') as file:\n",
    "    reader = csv.reader(file, delimiter = '\\t')\n",
    "  # Read first 10 lines\n",
    "    for i in range(10):\n",
    "        print(next(reader))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "U5uNuyGSxRql"
   },
   "source": [
    "The dataset has more than 500k questions! We are going to parse the full dataset and create a sample of 10k questions to experiment with in our models since BERT training & inference can be really slow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-03-08T20:11:48.116359Z",
     "iopub.status.busy": "2023-03-08T20:11:48.115412Z",
     "iopub.status.idle": "2023-03-08T20:11:49.265583Z",
     "shell.execute_reply": "2023-03-08T20:11:49.264578Z",
     "shell.execute_reply.started": "2023-03-08T20:11:48.116324Z"
    },
    "id": "6_mRCola0s8z"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique questions: 10000\n",
      "Number of question with duplicates: 3810\n",
      "Number of questions in sample: 10000\n",
      "Number of duplicate pairs in sample: 3589\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Util function to parse the file\n",
    "\"\"\"\n",
    "def parse_sample_dataset(file_path, sample_max_id):\n",
    "  \"\"\"\n",
    "  Inputs:\n",
    "    file_path: Path to the raw data file\n",
    "    sample_max_id: Max question id to be considered in the sampled dataset\n",
    "\n",
    "  Returns 4 objects:\n",
    "    1. QuestionMap: list of all question ids\n",
    "    2. DuplicatesMap: Map of questionID to it's duplicates\n",
    "    3. SampleDataset: list of questionIds in the sample\n",
    "    4. SampleEvalDataset: list of pair of duplicate questions in the sample\n",
    "  \"\"\"\n",
    "  question_map = {}\n",
    "  duplicates_map = defaultdict(set)\n",
    "  sample_dataset = set([])\n",
    "  sample_eval_dataset = []\n",
    "\n",
    "  with open(file_path, 'r', newline='\\n', encoding='utf-8') as file:\n",
    "    reader = csv.reader(file, delimiter='\\t')\n",
    "    next(reader)  # Skip the header line\n",
    "\n",
    "    for row in reader:\n",
    "      if len(row) != 6: # Skip incomplete rows\n",
    "        continue\n",
    "\n",
    "      # Limit the sample size of the dataset at max_id\n",
    "      # Make sure all 4 objects start at index 0\n",
    "      qid1, qid2, label = int(row[1]) - 1, int(row[2]) - 1, int(row[5])\n",
    "      if qid1 < sample_max_id and qid2 < sample_max_id:\n",
    "        \n",
    "        if qid1 not in question_map:\n",
    "          question_map[qid1] = str(row[3])\n",
    "        if qid2 not in question_map:\n",
    "          question_map[qid2] = str(row[4])\n",
    "\n",
    "        if label == 1:\n",
    "          duplicates_map[qid1].add(qid2)\n",
    "          duplicates_map[qid2].add(qid1)\n",
    "\n",
    "          sample_eval_dataset.append((qid1, qid2))\n",
    "\n",
    "        sample_dataset.add(qid1)\n",
    "        sample_dataset.add(qid2)\n",
    "\n",
    "  # sample dataset duplicates removed via set(), so turn back into list\n",
    "  return question_map, duplicates_map, list(sample_dataset), sample_eval_dataset\n",
    "\n",
    "question_map, duplicates_map, sample_dataset, sample_eval_dataset, = parse_sample_dataset(DATA_FILE, 10000)\n",
    "\n",
    "# Complete file: 537k unique questions, 400k duplicate.\n",
    "# To keep training time manageable limited to 10.000 (sample_max_id)\n",
    "print(\"Number of unique questions:\", len(question_map)) # 10.000\n",
    "print(\"Number of question with duplicates:\", len(duplicates_map)) # ~3.8k\n",
    "print(\"Number of questions in sample:\", len(sample_dataset)) # 10.000\n",
    "print(\"Number of duplicate pairs in sample:\", len(sample_eval_dataset)) # ~3.6k"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tC9D41185-Oa"
   },
   "source": [
    "# Retrieval using Faiss -- TO BE COMPLETED\n",
    "\n",
    "You are now going to create an Indexer class that implements multiple functions for indexing, searching, and evaluating our retrieval model. Faiss documentation can be found in the wiki here: https://github.com/facebookresearch/faiss/wiki/Getting-started\n",
    "\n",
    "Some helpful Faiss guides are:\n",
    "- https://www.pinecone.io/learn/faiss-tutorial/\n",
    "- https://www.pinecone.io/learn/vector-indexes/\n",
    "\n",
    "You need to implement the following functions:\n",
    "\n",
    "1. **search**: Implement a function that takes a question and top_k variable and returns either the matched strings or the ids to the user as a \n",
    "    1. Call the search API on the faiss_index to look up similar sentences using `faiss_index.search`\n",
    "    2. Parse the output to either return [sentence_id, score] tuples or [sentence, score] tuples based on the input parameter\n",
    "    3. Sort the output by the score in descending order\n",
    "\n",
    "1. **evaluate**: Sample num_docs pairs from the evaluation dataset and then check if the qid2 is present in the top-k results\n",
    "    1. For each eval sample, find the top_k matches for the qid1\n",
    "    2. See if the qid2 is in one of the matches\n",
    "    3. If yes, append (1) to the recall array otherwise append (0)\n",
    "    4. Implement MRR (Mean reciprocal rank) addition based on the position of qid2 in matches.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the FAISS documentation (https://github.com/facebookresearch/faiss/wiki/Getting-started):\n",
    "Faiss handles collections of vectors of a fixed dimensionality d, typically a few 10s to 100s. \n",
    "These collections can be stored in matrices. We assume row-major storage, ie. the j'th component of vector number i \n",
    "is stored in row i, column j of the matrix. Faiss uses only 32-bit floating point matrices.\n",
    "\n",
    "We need two matrices:\n",
    "\n",
    "    xb for the database, that contains all the vectors that must be indexed, and that we are going to search in. Its size is nb-by-d\n",
    "    xq for the query vectors, for which we need to find the nearest neighbors. Its size is nq-by-d. If we have a single query vector, nq=1.\n",
    "\n",
    "In our case, we have\n",
    "d = sentence_vector_dim\n",
    "nb = 10000\n",
    "nq = 1  \n",
    "\n",
    "We have nq=1 since we will be querying a single sentence at a time-- we are asking: \"Given any sentence (typed by the user)return a list of \n",
    "top_k(sentence, sim_score) or top_k(sentence_ids, sim_score).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-03-08T20:31:30.887474Z",
     "iopub.status.busy": "2023-03-08T20:31:30.886926Z",
     "iopub.status.idle": "2023-03-08T20:31:30.913151Z",
     "shell.execute_reply": "2023-03-08T20:31:30.912065Z",
     "shell.execute_reply.started": "2023-03-08T20:31:30.887428Z"
    },
    "id": "Ji7yyBk5Ou6k"
   },
   "outputs": [],
   "source": [
    "class FaissIndexer:\n",
    "  def __init__(self, dataset,\n",
    "               question_map, \n",
    "               eval_dataset, \n",
    "               batch_size, \n",
    "               sentence_vector_dim, \n",
    "               vectorizer):\n",
    "    self.question_map = question_map\n",
    "    self.dataset = dataset\n",
    "    self.eval_dataset = eval_dataset\n",
    "    self.batch_size = batch_size\n",
    "    self.vectorizer = vectorizer\n",
    "    self.sentence_vector_dim = sentence_vector_dim\n",
    "    \n",
    "    \n",
    "    # Want to use a GPU, going to initalize an empty one\n",
    "    # instantiate which index I'm using in the index definition\n",
    "    # when doing the index.\n",
    "    \n",
    "    self.faiss_index = None\n",
    "    self.res = faiss.StandardGpuResources()\n",
    "    \n",
    "    # To deal with GPU memory issues\n",
    "    torch.cuda.empty_cache()\n",
    "    self.res.noTempMemory()\n",
    "    \n",
    "    # This below was what I originally was doing, wasn't working for a few days, but want to investigate this more\n",
    "    # everywhere I was reading they were saying this is how you take your already loaded index and move it over\n",
    "    # I instead had to just initiate a blank one create faiss.GpuIndexFlatIP and rename it, although this might have\n",
    "    # not been optimal\n",
    "    #self.faiss_index = faiss.index_cpu_to_gpu(self.res, 0, self.flat_index)\n",
    "    \n",
    "    \n",
    "    \n",
    "  def split_list(self, lst: list, sublist_size: int):\n",
    "    sublists = []\n",
    "    # Split lst into even chunks/sublists/batches\n",
    "    for i in range(0, len(lst), sublist_size): \n",
    "        sublists.append(lst[i:i + sublist_size])\n",
    "    return sublists\n",
    "\n",
    "\n",
    "  def index(self):\n",
    "    \"\"\"\n",
    "    This funtion adds all the sentences in the dataset to the faiss index.\n",
    "    It first splits the dataset into batches of size batch_size.\n",
    "    Then it retrieves the sentences from the question_map, and vectorizes them.\n",
    "    After adding to a temporary list, it adds all the batches to the faiss index.\n",
    "    \n",
    "    \"\"\"\n",
    "    sentence_vectors = []\n",
    "\n",
    "    print(\"Start indexing!\")\n",
    "    for sentence_ids in tqdm(self.split_list(self.dataset, self.batch_size)):\n",
    "      # Retrieve sentences based on qid\n",
    "      sentences = [question_map[qid] for qid in sentence_ids]\n",
    "      # Get embeddings of the sentences (Spacy, ..., Cohere)\n",
    "      sentence_vectors_batch = self.vectorizer.vectorize(sentences)\n",
    "      # Add batch to temporary list\n",
    "      sentence_vectors.append(cp.asarray(sentence_vectors_batch)) ## Changed (sentence_vectors_batch) to cp.asarray(sentence_vectors_batch)\n",
    "      \n",
    "\n",
    "    # Add all batches from temporary list to index. faiss_index takes np arrays\n",
    "    concatenated_vectors = cp.concatenate(sentence_vectors, axis=0)\n",
    "    # Need to turn them into numpy vectors before adding to index\n",
    "    concatenated_vectors_np = cp.asnumpy(concatenated_vectors)\n",
    "    \n",
    "    # FlatIP uses Inner Product distance\n",
    "    # (https://github.com/facebookresearch/faiss/blob/main/tutorial/python/4-GPU.py)\n",
    "    gpu_index = faiss.GpuIndexFlatIP(self.res, self.sentence_vector_dim)\n",
    "    gpu_index.add(concatenated_vectors_np)\n",
    "    \n",
    "    self.faiss_index = gpu_index\n",
    "    \n",
    "\n",
    "\n",
    "  def search(self, question: str, top_k: int, return_ids=False):\n",
    "    \"\"\"Given any sentence (typed by the user)\n",
    "    We return a list of top_k(sentence, sim_score) or top_k(sentence_ids, sim_score)\n",
    "    \n",
    "    NOTE: The output type is controlled by the return_ids flag\n",
    "\n",
    "    1. Call the search API on the faiss_index to look up similar sentences \n",
    "       using `faiss_index.search`\n",
    "    2. Parse the output to either return [sentence_id, score] tuples or \n",
    "       [sentence, score] tuples based on return_ids being true/false\n",
    "    3. Sort the output by the score in descending order\n",
    "    \"\"\"\n",
    "\n",
    "    # NOTE: We converted the question to a list here to match the signature \n",
    "    # of the vectorize function\n",
    "    question_vectors = self.vectorizer.vectorize([question])\n",
    "    question_vectors_np = cp.asnumpy(question_vectors)\n",
    "\n",
    "    ### TO BE IMPLEMENTED ###\n",
    "    # 1. Call the search API on the faiss_index to look up similar sentences using `faiss_index.search`\n",
    "    \n",
    "    # Here, D contains the distances between the query vector and the nearest neighbors and I contains their\n",
    "    # corresponding indices in the lookup table.\n",
    "    # self.faiss_index.search needs numpy vectors. I'll convert back after\n",
    "    D, I = self.faiss_index.search(question_vectors_np, top_k)\n",
    "    #D, I = self.faiss_index.search(question_vectors, top_k)\n",
    "    \n",
    "    # 2. Parse the output to either return [sentence_id, score] tuples or [sentence, score] tuples based on return_ids being true/false\n",
    "    # Earlier, we created a dictionary called question_map that maps the qid to the sentence.\n",
    "    # As we are only putting a single question in the question_vectors, there is only one element in the D and I numpy arrays.\n",
    "    \n",
    "    if return_ids:\n",
    "      output = [(i, d) for d, i in zip(D[0], I[0])]\n",
    "    else:\n",
    "      output = [(self.question_map[i], d) for d, i in zip(D[0], I[0])]\n",
    "    \n",
    "\n",
    "    \n",
    "    # 3. Sort the output by the score in descending order\n",
    "\n",
    "    # output is a List[(q, score), (q, score), (q, score)] based on return_ids\n",
    "    # Additionally, output is sorted in descending order based on score\n",
    "    # We want to sort on the scores, so we can use the sorted() function\n",
    "    # with a lambda function as the key parameter that returns the second\n",
    "    # element.\n",
    "    return sorted(output, key=lambda q: q[1], reverse=True)\n",
    "\n",
    "\n",
    "  def evaluate(self, top_k: int, eval_sample_size: int):\n",
    "    \"\"\"Sample num_docs pairs from the evaluation dataset and then check \n",
    "    if the qid2 is present in the top-k results\n",
    "\n",
    "    1. For each eval sample, find the top_k matches for the qid1\n",
    "    2. See if the qid2 is in one of the matches\n",
    "    3. If yes, append (1) to the recall array otherwise append (0)\n",
    "    4. Implement MRR (Mean reciprocal rank) addition based on the position of qid2 in matches\n",
    "      - Note: MRR is equivalent to mean([1/r or 0 for each sample])\n",
    "    \"\"\"\n",
    "    # Sample from evaluation dataset as proxy for performance metrics\n",
    "    eval_samples = random.sample(self.eval_dataset, eval_sample_size)\n",
    "    # Retrieval metrics which only care about if searched for\n",
    "    # item is present among the results.\n",
    "    recall_at_k = [] # Relevant items vs total of relevant items\n",
    "    mean_reciprocal_rank = [] # Rank of the first relevant item\n",
    "    \n",
    "    for q1, q2 in eval_samples:\n",
    "        query = self.question_map[q1]\n",
    "        top_k_similar = self.search(query, top_k, return_ids=True)\n",
    "        # Get the IDs of the top_k sentences closest to query\n",
    "        # and put them in a list\n",
    "        top_k_ids = [id for (id, d) in top_k_similar]\n",
    "        if q2 in top_k_ids:\n",
    "          recall_at_k.append(1)\n",
    "          mean_reciprocal_rank.append(1 / (top_k_ids.index(q2) + 1))\n",
    "        else:\n",
    "          recall_at_k.append(0)\n",
    "          mean_reciprocal_rank.append(0)\n",
    "    \n",
    "\n",
    "    print(\"\\nRecall@{}:\\t\\t{:0.2f}%\".format(top_k, np.mean(np.array(recall_at_k) * 100.0)))\n",
    "    print(\"Mean Reciprocal Rank:\\t{:0.2f}\".format(np.mean(np.array(mean_reciprocal_rank))))\n",
    "\n",
    "\n",
    "  # Helper function to train, search and evaluate similar output from all the models created.\n",
    "  def train_and_evaluate(self, \n",
    "                         question_example: str, \n",
    "                         top_k: int = 10, \n",
    "                         eval_sample_size: int = 1000\n",
    "                         ):\n",
    "    print(\"---- Indexing ----\")\n",
    "    self.index()\n",
    "    print(\"\\n---- Search ----\")\n",
    "    results = self.search(question_example, top_k, return_ids=False)\n",
    "    print(\"Questions similar to:\", question_example)\n",
    "    for i, (q, s) in enumerate(results):\n",
    "      print(f\"{i} Question: {q} with score {s:.4f}\")\n",
    "    print(\"\\n---- Evaluation ----\")\n",
    "    self.evaluate(top_k, eval_sample_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kuMWzJZjpKdW"
   },
   "source": [
    "## Dummy Model Test\n",
    "\n",
    "Really small sample of 4 sentences to make sure we can test our implementation of the FAISS search function correctly. We just project the 4 questions in a 2-d space where they are placed on the X-Axis if the word `invest` is present and on the Y-axis if `kohinoor` is present. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-03-08T20:11:49.289228Z",
     "iopub.status.busy": "2023-03-08T20:11:49.288875Z",
     "iopub.status.idle": "2023-03-08T20:11:49.295703Z",
     "shell.execute_reply": "2023-03-08T20:11:49.294614Z",
     "shell.execute_reply.started": "2023-03-08T20:11:49.289201Z"
    },
    "id": "1UmvNFIIw1eO"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Questions:\n",
      "0 : What is the step by step guide to invest in share market in india?\n",
      "1 : What is the step by step guide to invest in share market?\n",
      "2 : What is the story of Kohinoor (Koh-i-Noor) Diamond?\n",
      "3 : What would happen if the Indian government stole the Kohinoor (Koh-i-Noor) diamond back?\n"
     ]
    }
   ],
   "source": [
    "dummy_ids = sample_dataset[:4]\n",
    "print(\"Questions:\")\n",
    "for i in dummy_ids:\n",
    "  print(i, \":\", question_map[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-03-08T20:11:49.298446Z",
     "iopub.status.busy": "2023-03-08T20:11:49.297651Z",
     "iopub.status.idle": "2023-03-08T20:11:50.707529Z",
     "shell.execute_reply": "2023-03-08T20:11:50.706527Z",
     "shell.execute_reply.started": "2023-03-08T20:11:49.298411Z"
    },
    "id": "x8n4UBEZpPT1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start indexing!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 592.50it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Questions similar to: invest\n",
      "0 Question: What is the step by step guide to invest in share market? with score 0.275837242603302\n",
      "1 Question: What is the step by step guide to invest in share market in india? with score 0.21480777859687805\n",
      "2 Question: What would happen if the Indian government stole the Kohinoor (Koh-i-Noor) diamond back? with score 0.0\n",
      "3 Question: What is the story of Kohinoor (Koh-i-Noor) Diamond? with score 0.0\n",
      "\n",
      "Questions similar to: Kohinoor\n",
      "0 Question: What would happen if the Indian government stole the Kohinoor (Koh-i-Noor) diamond back? with score 0.0271025151014328\n",
      "1 Question: What is the story of Kohinoor (Koh-i-Noor) Diamond? with score 0.004366403911262751\n",
      "2 Question: What is the step by step guide to invest in share market in india? with score 0.0\n",
      "3 Question: What is the step by step guide to invest in share market? with score 0.0\n"
     ]
    }
   ],
   "source": [
    "class DummyVectorizer:\n",
    "  def __init__(self, sentence_vector_dim):\n",
    "    self.sentence_vector_dim = sentence_vector_dim\n",
    "\n",
    "  def vectorize(self, sentences):\n",
    "    \"\"\"Return sentence vectors for the batch of sentences. \n",
    "\n",
    "    1. Tokenize each sentence and create vectors for each token in the sentence\n",
    "    2. Sentence vector is the mean of word vectors of each token\n",
    "    3. Stack the sentence vectors into a numpy array using np.stack\n",
    "    \"\"\"\n",
    "    vectors = []\n",
    "    for sentence in sentences:\n",
    "      if \"invest\" in sentence:\n",
    "        # If \"invest\" is present place it on the X-Axis\n",
    "        # In this example block, we are feeding 4 sentences, two of which have \n",
    "        # the word 'invest', and each time we get inside this loop, we\n",
    "        # create a one-dimensional CuPy array of length 2 with a randomly \n",
    "        # generated float value between 0 and 1 at index 0, and a float value \n",
    "        # of 0 at index 1, and append it to vectors. \n",
    "        vectors.append(cp.array([random.random(), 0], dtype=cp.float32))\n",
    "      elif \"Kohinoor\" in sentence:\n",
    "        # If \"Kohinoor\" is present place it on the Y-Axis. Same thing as above, except\n",
    "        # now we are getting a random y-value coordinate each time our sentence\n",
    "        # has \"Kohinoor\" in the sentence\n",
    "        vectors.append(cp.array([0, random.random()], dtype=cp.float32))\n",
    "    return cp.stack([v for v in vectors])\n",
    "\n",
    "\n",
    "di = FaissIndexer(dummy_ids, \n",
    "                  question_map,\n",
    "                  sample_eval_dataset,\n",
    "                  batch_size=1024, \n",
    "                  sentence_vector_dim=2, \n",
    "                  vectorizer=DummyVectorizer(2)\n",
    "                  )\n",
    "\n",
    "di.index()\n",
    "\n",
    "results = di.search(\"invest\", 4)\n",
    "print(\"Questions similar to:\", \"invest\")\n",
    "for i, (q, s) in enumerate(results):\n",
    "  print(f\"{i} Question: {q} with score {s}\")\n",
    "\n",
    "results = di.search(\"Kohinoor\", 4)\n",
    "print(\"\\nQuestions similar to:\", \"Kohinoor\")\n",
    "for i, (q, s) in enumerate(results):\n",
    "  print(f\"{i} Question: {q} with score {s}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EtZuKF-Y3kkB"
   },
   "source": [
    "# Models\n",
    "\n",
    "You may be wondering, \"When are we going to start building models?\" And, the answer is NOW! Finally the time has come to build our baseline model, and then we'll work towards improving it. \n",
    "\n",
    "\n",
    "**NOTE**: We will be using the sample dataset since BERT is really slow and processing the full dataset will take a lot of time. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vFesUhmewgGY"
   },
   "source": [
    "### Model 1: Averaging Word Vectors --- TO BE COMPLETED\n",
    "##### <font color='red'>Expected recall@10: ~20%, MRR: ~0.07</font>\n",
    "\n",
    "Complete the `vectorize` function using Spacy provided word embeddings. This is something we've done twice already :) \n",
    "\n",
    "Implementation:\n",
    "\n",
    "1. Tokenize each sentence and get wordVectors for each token in the sentence using Spacy \n",
    "2. Sentence vector is the mean of word vectors of each token\n",
    "3. Stack the sentence vectors into a numpy array using np.stack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-03-08T20:11:50.709352Z",
     "iopub.status.busy": "2023-03-08T20:11:50.709013Z",
     "iopub.status.idle": "2023-03-08T20:11:50.716700Z",
     "shell.execute_reply": "2023-03-08T20:11:50.715087Z",
     "shell.execute_reply.started": "2023-03-08T20:11:50.709324Z"
    },
    "id": "1ahi3pH_Ce6c"
   },
   "outputs": [],
   "source": [
    "class SpacyVectorizer:\n",
    "  def __init__(self, sentence_vector_dim):\n",
    "    self.sentence_vector_dim = sentence_vector_dim\n",
    "\n",
    "  def vectorize(self, sentences):\n",
    "    \"\"\"Return sentence vectors for the batch of sentences. \n",
    "\n",
    "    1. Tokenize each sentence and create vectors for each token in the sentence\n",
    "    2. Sentence vector is the mean of word vectors of each token\n",
    "    3. Stack the sentence vectors into a numpy array using np.stack\n",
    "    \"\"\"\n",
    "    vectors = []\n",
    "    for sentence in sentences:\n",
    "      spacy_doc = encoder(sentence)\n",
    "      tokens = [token.vector for token in spacy_doc]\n",
    "      sentence_vector = cp.mean(cp.asarray(tokens), axis=0)\n",
    "      vectors.append(sentence_vector)\n",
    "      \n",
    "    return cp.stack(vectors) \n",
    "\n",
    "\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-03-08T20:37:23.849479Z",
     "iopub.status.busy": "2023-03-08T20:37:23.848050Z",
     "iopub.status.idle": "2023-03-08T20:41:27.620571Z",
     "shell.execute_reply": "2023-03-08T20:41:27.619568Z",
     "shell.execute_reply.started": "2023-03-08T20:37:23.849412Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---- Indexing ----\n",
      "Start indexing!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [03:40<00:00, 22.04s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "---- Search ----\n",
      "Questions similar to: how can i invest in stock market in india?\n",
      "0 Question: In how many ways can we create object in Java? with score 1264.3943\n",
      "1 Question: How can we find happiness in life? with score 1203.2908\n",
      "2 Question: I want to connect with you, how can I do that? with score 1198.1931\n",
      "3 Question: How can I can concentrate well in studies? with score 1197.0286\n",
      "4 Question: what can i do to become fair? with score 1191.5201\n",
      "5 Question: Why do we need to study? with score 1189.1204\n",
      "6 Question: How can we earn money online in india? with score 1163.9592\n",
      "7 Question: How can I be happy if I don't have any reason to be? with score 1163.7311\n",
      "8 Question: What can I do to help the situation in Aleppo? with score 1161.9705\n",
      "9 Question: I want to study biotechnology abroad what do I have to do? with score 1161.9376\n",
      "\n",
      "---- Evaluation ----\n",
      "\n",
      "Recall@10:\t\t5.40%\n",
      "Mean Reciprocal Rank:\t0.02\n",
      "Elapsed time: 243.7632293701172 seconds\n"
     ]
    }
   ],
   "source": [
    "start_time=time.time()\n",
    "\n",
    "spacyIndexer = FaissIndexer(sample_dataset,\n",
    "                            question_map,\n",
    "                            sample_eval_dataset,\n",
    "                            batch_size=1024, \n",
    "                            sentence_vector_dim=300, \n",
    "                            vectorizer=SpacyVectorizer(300))\n",
    "\n",
    "\n",
    "\n",
    "spacyIndexer.train_and_evaluate(question_example = \"how can i invest in stock market in india?\")\n",
    "\n",
    "elapsed_time = time.time() - start_time\n",
    "\n",
    "print(f\"Elapsed time: {elapsed_time} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EHoRuwCOwhiH"
   },
   "source": [
    "### Model 2: BERT Embeddings --- TO BE COMPLETED\n",
    "##### <font color='red'>Expected recall@10: ~48%, MRR: ~0.19</font>\n",
    "\n",
    "Compute the sentence embeddings using the BERT model and complete the `vectorize` function. Feel free to reference any documentation from https://huggingface.co/. \n",
    "\n",
    "\n",
    "Implementation:\n",
    "\n",
    "1. Tokenize batch of sentences using `self.tokenizer`\n",
    "2. Pipe the inputs through the BERT model to create the output logits\n",
    "3. Normalize the batch output\n",
    "\n",
    "**NOTE: This model is really slow and will take about 20 mins to run**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-03-08T20:31:40.192757Z",
     "iopub.status.busy": "2023-03-08T20:31:40.191433Z",
     "iopub.status.idle": "2023-03-08T20:32:07.513356Z",
     "shell.execute_reply": "2023-03-08T20:32:07.512225Z",
     "shell.execute_reply.started": "2023-03-08T20:31:40.192694Z"
    },
    "id": "mAWOt3cTC9Ig"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertModel: ['vocab_transform.bias', 'vocab_projector.weight', 'vocab_layer_norm.bias', 'vocab_transform.weight', 'vocab_layer_norm.weight', 'vocab_projector.bias']\n",
      "- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---- Indexing ----\n",
      "Start indexing!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1250/1250 [00:15<00:00, 79.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "---- Search ----\n",
      "Questions similar to: how can i invest in stock market in india?\n",
      "0 Question: What is the step by step guide to invest in share market in india? with score 0.8995\n",
      "1 Question: What are mutual funds and which is the best one in India in which to invest? with score 0.8811\n",
      "2 Question: I wish to start investing in Equity and Mutual Funds. Where should I open Demat account for best rates, transaction charges and so on? I am NRI. with score 0.8783\n",
      "3 Question: What should I do to make money online in India? with score 0.8765\n",
      "4 Question: What is the best time to withdraw money from working ATM in present India? with score 0.8727\n",
      "5 Question: Do you think India will be able to curb blank money? with score 0.8709\n",
      "6 Question: What will be the effect of banning 500 and 1000 notes on stock markets in India? with score 0.8670\n",
      "7 Question: How can I start up a small business? with score 0.8656\n",
      "8 Question: How can I make money online in India? with score 0.8642\n",
      "9 Question: How do micro ATMs work? Which bank introduced them in India? with score 0.8620\n",
      "\n",
      "---- Evaluation ----\n",
      "\n",
      "Recall@10:\t\t56.20%\n",
      "Mean Reciprocal Rank:\t0.23\n",
      "Elapsed time: 25.927167415618896 seconds\n"
     ]
    }
   ],
   "source": [
    "class BertVectorizer:\n",
    "  def __init__(self):\n",
    "    self.tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')\n",
    "    self.model = DistilBertModel.from_pretrained('distilbert-base-uncased')\n",
    "    self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    self.model.to(self.device)\n",
    "\n",
    "  def vectorize(self, sentences):\n",
    "    \"\"\"Return sentence vectors for the batch of sentences. \n",
    "\n",
    "    1. Tokenize batch of sentences using `self.tokenizer`\n",
    "    2. Pipe the inputs through the BERT model to create the output logits\n",
    "    3. Normalize the batch output\n",
    "    \"\"\"\n",
    "    # We want to tokenize our sentence vectors with DistilBert, return them\n",
    "    # as Pytorch tensors, and apply padding so that all the tokenized\n",
    "    # inputs come out to be the same length (as we did in previous projects)\n",
    "    # BERT needs all inputs to be the same size.\n",
    "    tokenized = self.tokenizer(sentences, return_tensors='pt', padding=True).to(self.device)\n",
    "    model_output = self.model(**tokenized).last_hidden_state\n",
    "    \n",
    "\n",
    "    return F.normalize(torch.mean(model_output, dim=1), dim=1).detach()\n",
    "\n",
    "start_time=time.time()\n",
    "\n",
    "bertIndex = FaissIndexer(sample_dataset,\n",
    "                  question_map,\n",
    "                  sample_eval_dataset,\n",
    "                  batch_size=8, \n",
    "                  sentence_vector_dim=768, \n",
    "                  vectorizer=BertVectorizer())\n",
    "\n",
    "\n",
    "bertIndex.train_and_evaluate(question_example = \"how can i invest in stock market in india?\")\n",
    "\n",
    "elapsed_time = time.time() - start_time\n",
    "\n",
    "print(f\"Elapsed time: {elapsed_time} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4bLdQ4pQwo-m"
   },
   "source": [
    "### Model 3: Sentence Transformer --- TO BE COMPLETED\n",
    "##### <font color='red'>Expected recall@10: ~93%, MRR: ~0.34</font>\n",
    "\n",
    "Compute the sentence embeddings using the Sentence BERT model and complete the `vectorize` function. Feel free to look up documentation on https://www.sbert.net/. \n",
    "\n",
    "Implementation:\n",
    "\n",
    "1. Pipe the input sentences through the Sentence BERT model to create the output logits\n",
    "2. Normalize the batch output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-03-08T20:32:54.443491Z",
     "iopub.status.busy": "2023-03-08T20:32:54.442702Z",
     "iopub.status.idle": "2023-03-08T20:33:08.663764Z",
     "shell.execute_reply": "2023-03-08T20:33:08.662720Z",
     "shell.execute_reply.started": "2023-03-08T20:32:54.443444Z"
    },
    "id": "IpTt--KFTd3t"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---- Indexing ----\n",
      "Start indexing!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [00:03<00:00,  5.48it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "---- Search ----\n",
      "Questions similar to: how can i invest in stock market in india?\n",
      "0 Question: What is the step by step guide to invest in share market in india? with score 0.7332\n",
      "1 Question: I am 17 and I want to invest money in stock market where should I start? with score 0.6957\n",
      "2 Question: What are the ways to learn about stock market? with score 0.6244\n",
      "3 Question: How do I start investing in shares or stocks? What is the minimum requirement? with score 0.6240\n",
      "4 Question: What is the best way to learn about stock market? with score 0.6223\n",
      "5 Question: What is the step by step guide to invest in share market? with score 0.6043\n",
      "6 Question: What is the best way to learn about investing in the stock market and what stocks to buy? with score 0.6033\n",
      "7 Question: What is the best way to learn about stock markets? with score 0.5847\n",
      "8 Question: How do I buy stocks? with score 0.5778\n",
      "9 Question: What are mutual funds and which is the best one in India in which to invest? with score 0.5577\n",
      "\n",
      "---- Evaluation ----\n",
      "\n",
      "Recall@10:\t\t91.80%\n",
      "Mean Reciprocal Rank:\t0.34\n"
     ]
    }
   ],
   "source": [
    "class SentenceBertVectorizer:\n",
    "  def __init__(self):\n",
    "    self.model = SentenceTransformer('paraphrase-MiniLM-L6-v2')\n",
    "\n",
    "  def vectorize(self, sentences):\n",
    "    \"\"\"Return sentence vectors for the batch of sentences. \n",
    "\n",
    "    1. Pipe the input sentences through the Sentence BERT model to create the output logits\n",
    "    2. Normalize the batch output\n",
    "    \"\"\"\n",
    "\n",
    "    sentence_vectors = cp.asarray(self.model.encode(sentences))\n",
    "    norm = cp.linalg.norm(sentence_vectors, axis=1, keepdims=True)\n",
    "    normalized_vectors = sentence_vectors / norm\n",
    "\n",
    "    return cp.asnumpy(normalized_vectors)\n",
    "\n",
    "start_time=time.time()\n",
    "SBertIndex = FaissIndexer(sample_dataset,\n",
    "                  question_map,\n",
    "                  sample_eval_dataset,\n",
    "                  batch_size=512, \n",
    "                  sentence_vector_dim=384, \n",
    "                  vectorizer=SentenceBertVectorizer())\n",
    "\n",
    "SBertIndex.train_and_evaluate(question_example = \"how can i invest in stock market in india?\")\n",
    "\n",
    "elapsed_time = time.time() - start_time\n",
    "\n",
    "print(f\"Elapsed time: {elapsed_time} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BHhRKpknq5we"
   },
   "source": [
    "### Model 4: Cohere Sentence Embeddings --- TO BE COMPLETED\n",
    "##### <font color='red'>Expected recall@10: ~89%, MRR: ~0.34</font>\n",
    "\n",
    "Make sure create a Cohere account and make an API key.\n",
    "Compute the sentence embeddings using the cohere API and complete the `vectorize` function. Feel free to look up documentation on https://docs.cohere.ai/semantic-search. \n",
    "\n",
    "Implementation:\n",
    "\n",
    "1. Pipe the input sentences through the Cohere API. Make sure to select the small model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-03-08T20:33:19.798306Z",
     "iopub.status.busy": "2023-03-08T20:33:19.797949Z",
     "iopub.status.idle": "2023-03-08T20:33:19.877039Z",
     "shell.execute_reply": "2023-03-08T20:33:19.875866Z",
     "shell.execute_reply.started": "2023-03-08T20:33:19.798275Z"
    },
    "id": "FODgVGURr2YP"
   },
   "outputs": [],
   "source": [
    "COHERE_API_KEY = \"\"\n",
    "co = cohere.Client(COHERE_API_KEY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-03-08T20:33:21.023775Z",
     "iopub.status.busy": "2023-03-08T20:33:21.023385Z",
     "iopub.status.idle": "2023-03-08T20:33:26.229915Z",
     "shell.execute_reply": "2023-03-08T20:33:26.228776Z",
     "shell.execute_reply.started": "2023-03-08T20:33:21.023740Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Collecting limit\n",
      "  Downloading limit-0.2.3.tar.gz (1.9 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hBuilding wheels for collected packages: limit\n",
      "  Building wheel for limit (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for limit: filename=limit-0.2.3-py3-none-any.whl size=2328 sha256=45dd2d0bb9fb0bcd532aa61a7c4918c7fe575d9d32a86b55708a106d5c11873c\n",
      "  Stored in directory: /root/.cache/pip/wheels/e9/84/f3/bb4995b5e0b313a9965e4fb6ef50c502f985850d5765e94f6b\n",
      "Successfully built limit\n",
      "Installing collected packages: limit\n",
      "Successfully installed limit-0.2.3\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "# Limit calls to the API (tips from people in Slack that notesd cohere\n",
    "# only allows 100 calls/min)\n",
    "\n",
    "!pip install limit\n",
    "\n",
    "from limit import limit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-03-08T20:41:27.622393Z",
     "iopub.status.busy": "2023-03-08T20:41:27.622123Z",
     "iopub.status.idle": "2023-03-08T20:54:47.667915Z",
     "shell.execute_reply": "2023-03-08T20:54:47.667272Z",
     "shell.execute_reply.started": "2023-03-08T20:41:27.622363Z"
    },
    "id": "QnYpcV8Cq5wk"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---- Indexing ----\n",
      "Start indexing!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 313/313 [03:07<00:00,  1.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "---- Search ----\n",
      "Questions similar to: how can i invest in stock market in india?\n",
      "0 Question: What is the step by step guide to invest in share market in india? with score 2562.9946\n",
      "1 Question: I am 17 and I want to invest money in stock market where should I start? with score 2064.2815\n",
      "2 Question: What is the step by step guide to invest in share market? with score 2049.3308\n",
      "3 Question: How do I start investing in shares or stocks? What is the minimum requirement? with score 1887.5629\n",
      "4 Question: Which is the best Mutual Fund in India? with score 1856.3855\n",
      "5 Question: I wish to start investing in Equity and Mutual Funds. Where should I open Demat account for best rates, transaction charges and so on? I am NRI. with score 1831.6359\n",
      "6 Question: How do I buy stocks? with score 1825.7646\n",
      "7 Question: What are mutual funds and which is the best one in India in which to invest? with score 1824.3256\n",
      "8 Question: Which Best SIP plan in india for investement purpose? with score 1824.1649\n",
      "9 Question: What are the ways to learn about stock market? with score 1807.7783\n",
      "\n",
      "---- Evaluation ----\n",
      "\n",
      "Recall@10:\t\t90.20%\n",
      "Mean Reciprocal Rank:\t0.33\n",
      "Elapsed time: 800.0396497249603 seconds\n"
     ]
    }
   ],
   "source": [
    "class CohereVectorizer:\n",
    "\n",
    "  @limit(95,60)\n",
    "  def vectorize(self, sentences):\n",
    "    \"\"\"Return sentence vectors for the batch of sentences. \n",
    "\n",
    "    1. Tokenize each sentence and create vectors for each token in the sentence\n",
    "    2. Sentence vector is the mean of word vectors of each token\n",
    "    3. Stack the sentence vectors into a numpy array using np.stack\n",
    "    \"\"\"\n",
    "\n",
    "    embeddings = co.embed(texts = sentences, model = \"small\", truncate = \"LEFT\").embeddings\n",
    "    sentence_vectors = np.array(embeddings).astype('float')\n",
    "\n",
    "    # Convert from float64 to float32 to prevent bug:\n",
    "    # https://github.com/facebookresearch/faiss/issues/461\n",
    "    return np.float32(np.stack(sentence_vectors))\n",
    "start_time=time.time()\n",
    "\n",
    "cohereIndex = FaissIndexer(sample_dataset,\n",
    "                  question_map,\n",
    "                  sample_eval_dataset,\n",
    "                  batch_size=32, \n",
    "                  sentence_vector_dim=1024, \n",
    "                  vectorizer=CohereVectorizer())\n",
    "\n",
    "\n",
    "\n",
    "cohereIndex.train_and_evaluate(question_example = \"how can i invest in stock market in india?\")\n",
    "\n",
    "elapsed_time = time.time() - start_time\n",
    "\n",
    "print(f\"Elapsed time: {elapsed_time} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "onEbNcqnq5wl"
   },
   "source": [
    "🎉 CONGRATULATIONS on finishing the assignment!!! We built a real model with an actual datasets for a problem that is used every time a new Quora question gets created!! \n",
    "\n",
    "As for why did SentenceBERT & Cohere perform so well, we'll cover that in Siamese networks in week4."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IM0pHSbtGHuj"
   },
   "source": [
    "# Extensions\n",
    "\n",
    "Now that you've worked through the project there is a lot more for us to try:\n",
    "\n",
    "- See if you can use BERT to improve the model you shipped in Week 1.\n",
    "- Try out `SentenceBert` and `SpacyVectors` on the entire dataset rather the sample and see what you get?\n",
    "- Try different transformer models from hugging face"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
